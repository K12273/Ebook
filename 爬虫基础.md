### 参考官网

地址:https://docs.python.org/3/library/urllib.parse.html#module-urllib.parse

### urllib.request

- 简单get页面爬取

```python
import urllib.request
response=urllib.request.urlopen('https://www.python.org')
print(response.read().decode('utf-8'))
print(response.status)
print(response.getheaders())
print(response.getheader('Server'))
```
`response`返回一个`HTTPRespnse对象`对象,这个对象拥有很多的方法 :

`read()`: 返回响应的网页内容

`status`: 返回响应的状态码

`getheaders()`: 返回响应头的所有信息

`getheader()` : 返回单个请求头的具体信息

待续。。。

```python
print(response.getheader('Server'))
```


***

#### `urlopen()`API方法及参数详解

```
urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)
```

- `url`完成基本简单页面的请求抓取，返回的是HTTPRespnse对象(这个对象有许多的方法与属性如`read()`)

- `data` 参数，字节流编码需要使用`bytes()`转化，用于`post` 方式提交。
事例如下:

```python
import urllib.parse
import urllib.request
# 构造请求的数据
data=bytes(urllib.parse.urlencode({'word':'hello'}),encoding='utf8') # 将参数的字段转化为字符窜。
response=urllib.request.urlopen('http://httpbin.org/post',data=data)
print(response.read())
```

- `timeout` 设置超时时间，单位为秒，超过设定的请求时间抛出异常。


```python
import urllib.request
import socket
import urllib.error
try:
    response=urllib.request.urlopen('http://httpbin.org/get',timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason,socket.timeout):#判断该异常类型是否为超时异常
        print('Time Out')
```
- `context` 必须是ssl.SSLContext类型，指定ssl设置

- `cafile` 指定ca证书

- `capath` 指定ca证书的路径

** 更详细的信息参照官方文档 真男人看官方文档**

***

#### Request 方法


- `urlopne()`方法的升级-`Request`,因为涉及到请求头信息的传递的话，Request方法可以满足。

```python
import urllib.request
#Request()实现请求
request=urllib.request.Request('https://python.org')
response=urllib.request.urlopen(request)# 还是可以通过urlopen发送请求，但是这次的url换成了request对象
print(response.read().decode('utf-8'))
```
#### Request方法API

```python
class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)
```
- `url` 参数url为请求的地址

- `data` post要提交的data数据，bytes转化为字节码

- `headers` 请求头

- `origin_req_host` 请求方的host地址或者ip地址

- `unverifiable` 表示该请求能否得到验证，`True`无法验证，`flase`可以被验证。

- `method` 请求的方法,`get`,`post`等

**案例:**

```python
from urllib import request,parse

url="http://httpbin.org/post" #post请求
# 构造请求头
headers={
    'User-Agent':'Mozilla/4.0(compatible;MSIE 5.5;Windows NT)',
    'Host':'httpbin.org'
}
# post要提交的数据
dict={
    'name':'Germey'
}
#
data=bytes(parse.urlencode(dict),encoding='utf8')
# 构造request请求
req=request.Request(url=url,data=data,headers=headers,method='POST')
# 获取响应信息
response=request.urlopen(req)
# 读取响应信息
print(response.read().decode('utf-8'))

```
**方法二**
```python
# 第二种添加请求头的方法

from urllib import request,parse

url="http://httpbin.org/post"
dict={
    'name':'Germey'
}
data=bytes(parse.urlencode(dict),encoding='utf8')
req=request.Request(url=url,data=data,method='POST')
req.add_header('User-Agent','Mozilla/4.0(compatible;MSIE 5.5;Windows NT)')
response=request.urlopen(req)
print(response.read().decode('utf-8'))
```

#### 高级扩展

**更高级的操作，比如cookies处理，代理设置等，需要更加高级的方法。**

**Handle工具登场**

#### `BaseHandler` 类

**基本方法**

- `default_open()`

- `protocol_request()`

**子类**

- `HTTPDefaultErrorHandler`:用于处理http响应错误，错误会抛出`HTTPError`类型的错误

- `HTTPRedirectHandler`:用于处理重定向

- `HTTPCookieProcessor`:用于处理Cookies

- `ProxyHandler`:用于设置代理，默认代理为空

- `HTTPPasswordMgr`:用于管理密码，他维护了用户名与密码表

- `HTTPBasicAuthHandler`:用于管理认证，如果一个链接打开的时候需要认证,那么可以用他来解决认证问题。

- `HTTPPasswordMgrWithDefaultRealm`:用于输入框的域的映射，None域的话默认会捕获所有的域。

#### openerDirector 类

- 称为`Opener` `urlopen()`就是urllib提供的一个opener。这里的话是在使用`handle`来构建opener

- handle中的opener有一个`open()`方法
**其他类参考官方文档**

```python
#涉及到登陆验证的
from urllib.request import HTTPPasswordMgrWithDefaultRealm,HTTPBasicAuthHandler,build_opener
from urllib.error import URLError
username='username'
password='password'
url='http://localhost:5000/'

p=HTTPPasswordMgrWithDefaultRealm()#实例化捕获域
p.add_password(None,url,username,password)#该域中添加用户名与密码
auth_handler=HTTPBasicAuthHandler(p)# 进行验证
opener=build_opener(auth_handler)#验证后构建opener
try:
    result=opener.open(url)#进行请求
    html=result.read().decode('utf-8')
    print(html)
except URLError as e:
    print(e.reason)
```
***

#### 代理

- 使用代理的方式

```python
from urllib.error import URLError
from urllib.request import ProxyHandler,build_opener
proxy_handler=ProxyHandler({ # 构建的代理池
    'http':'http://127.0.0.1:9043',
    'https':'http://127.0.0.1:9073',
})
opener=build_opener(proxy_handler)#构建opener
# 使用代理发送请求
# 使用open方法发送请求
try:
    response=opener.open('https://baidu.com')
    print(response.read().decode('utf-8'))
except URLError as e:
    print(e.reason)
```
- 获取网站的cookie信息

```python
import http.cookiejar,urllib.request
cookie=http.cookiejar.CookieJar()# 声明该cookiejar对象
handler=urllib.request.HTTPCookieProcessor(cookie)#使用该对象构造handle对象
opener=urllib.request.build_opener(handler)#再使用handle对象构造opener对象
response=opener.open('http://www.baidu.com')#调用opener.open方法执行请求
for item in cookie:
    print(item.name+"="+item.value)
```

- 将cookie保存为文件

```python
import http.cookiejar,urllib.request
filename='cookies.txt' #文件名
cookie=http.cookiejar.MozillaCookieJar(filename)#生成cookie文件的话需要使用MozillaCookieJar（cookiejar的子类）
handler=urllib.request.HTTPCookieProcessor(cookie)#生产handle对象
opener=urllib.request.build_opener(handler)#构造opener
response=opener.open('http://www.baidu.com')#发送请求
cookie.save(ignore_discard=True,ignore_expires=True)#cookie保存
```
- 保存为`LWPCookieJar` 格式的

```python
cookie=http.cookiejar.LWPCookieJar(filename)#生成cookie文件的话需要使用LWPCookieJar（cookiejar的子类）
```
- 读物cookie文件

```python
# 读取生成的cookie文件内容
# 以LWPCookieJar为例
import http.cookiejar,urllib.request
cookie=http.cookiejar.LWPCookieJar()
cookie.load('cookies333.txt',ignore_discard=True,ignore_expires=True) #通过load方法加载设置
handler=urllib.request.HTTPCookieProcessor(cookie)
opener=urllib.request.build_opener(handler)
response=opener.open('http://www.baidu.com')
print(response.read().decode('utf-8'))
```
***

### 处理异常

#### `URLError` 模块

**属性`reason`,归属于`urllib.error`模块**

```python
from urllib import request,error
try:
    response=request.urlopen('https://cuiqingcai.com/index.html')
except error.URLError as e:
    print(e.reason)
```
#### `HTTPError` 模块

- `URLError` 的子类,专门用来处理HTTP请求的错误,属性如下:

- `code` 返回http状态码

- `reason` 返回错误的原因

- `headers` 返回请求头

```python
from urllib import request,error
try:
    response=request.urlopen('https://cuiqingcai.com/index.html')
except error.HTTPError as e:
    print(e.reason,e.code,e.headers)
```

- 优化建议:先捕获子类的错误，再捕获父类的错误

```python
from urllib import request,error
try:
    response=request.urlopen('https://cuiqingcai.com/index.html')
except error.HTTPError as e:
    print(e.reason,e.code,e.headers)
except error.URLError as e:
    print(e.reason)
else:
    print('Request Successful')
```
- 有时候`reason`属性返回的不一定是字符窜，可能是`socket.time` 请求超时的对象，建议使用`isinstance` 这样的方法进行更详细的判断。

#### 解析链接

##### `urlparse()` 方法

- 实现url的识别与分段

```python
from urllib.parse import urlparse
result=urlparse('http://www.baidu.com/index.html;user?id=5#comment')
print(type(result),result)
```
- 返回结果如下

```python
<class 'urllib.parse.ParseResult'> ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')
```

- `urlparse` 参数详解

```python
urllib.parse.urlparse（urlstring，scheme =''，allow_fragments = True ）

# urlstring:url地址
# scheme 指定请求的协议 http或者https
# allow_fragments 是否忽略url参数
```
- urlstring 要解析的url地址

- scheme 指定的请求协议

- allow_fragments 是否解析参数

**案例**

```python
from urllib.parse import urlparse
#result=urlparse('http://www.baidu.com/index.html;usr?id=5#comment',allow_fragments=False)
#ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='usr', query='id=5#comment', fragment='')
result=urlparse('http://www.baidu.com/index.html#comment',allow_fragments=False)
#ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html#comment', params='', query='', fragment='')

# 当url中不包含params与query的时候，fragment部分被单位path的一部分
print(result)
```

- `urlsplit()` 方法 也是解析url，返回的是元组类型

```python
from urllib.parse import urlsplit

result=urlsplit('http://www.baidu.com/index.html;user?id=5#commet')
print(result)
```
- 结果如下

```python
SplitResult(scheme='http', netloc='www.baidu.com', path='/index.html;user', query='id=5', fragment='commet')
```

#### `urlunparse()` 方法

- 接收可迭代的对象，长度必须为6，否则抛出异常
- 拼接了url

```python
from urllib.parse import urlunparse
data=['http','www.baidu.com','index.html','user','a=6','comment']
print(urlunparse(data))
#http://www.baidu.com/index.html;user?a=6#comment
```
#### `urlunsplit`方法

```python
from urllib.parse import urlunsplit
data2=['http','www.baidu.com','index.html','a=6','comment']
print(urlunsplit(data2))
#urlunsplit运行结果
#http://www.baidu.com/index.html?a=6#comment
```
#### urljoin(base_url,new_url) 链接的合并方法

- base_url链接一般是带有协议，域名，和path路径,new_url对base_url没有的做出补充，有的话，new_url会覆盖base_url中已经有的部分

#### urlencode()序列化为get请求

- 接收字典的参数

```python
from urllib.parse import urlencode

params={
    'name':'tom',
    'age':22
}
base_url='http://www.baidu.com'
url=base_url+urlencode(params)
print(url)
```

#### 反序列化`parse_qs()`

- 只能反序列化参数部分

```python
from urllib.parse import parse_qs

params='name=tom&age=22'
print(parse_qs(params))
```

#### 反序列化`parse_qsl` 组成元组

#### quote() 将中文参数转化为url编码

#### unquote() 解码url参数

***

### 分析robots协议

- `robotparser` 解析robot.txt文件

- `RobotFileParser（url）`解析 要解析的robots.txt地址

#### 常用方法:

- `set_url()` robots.txt文件的链接

- `read()` 读取robots.txt文件

- `parse()` 按照robots协议解析规则

- `can_fetch()` 返回是否可以抓取这个url，参数一为`User-agent`,参数二为URL

- `mtime()` 返回上次抓取robots与分析的时间

- `modified()` 可以将当前时间设置为上次分析与抓取的时间。

**案例**

```python
from urllib.robotparser import RobotFileParser
rp=RobotFileParser()
rp.set_url('http://www.jianshu.com/robots.txt')
rp.read()
print(rp.can_fetch('*','http://www.jianshu.com/p/b67554025d7d'))
print(rp.can_fetch('*','http://www.jianshu.com/search?q=python&page=1&type=collections'))
```

## requests 库 升级中

1. 首先安装`requests`库

#### `get`方式请求地址

```python
import requests
# get请求地址
r=requests.get('https://www.baidu.com/')
print(type(r))#返回response对象
print(r.status_code) #获取状态码
print(type(r.text))#返回响应的内容
print(r.text)
print(r.cookies)#获取cookie对象
```

#### 其他请求方法

```python
#请他地址请求类型
r=requests.get('https://www.baidu.com/')
r=requests.post('https://www.baidu.com/post')
r=requests.put('https://www.baidu.com/put')
r=requests.delete('https://www.baidu.com/delete')
r=requests.head('https://www.baidu.com/get')
r=requests.options('https://www.baidu.com/get')
```

- 简单的get请求

```python
import requests
r=requests.get('http://httpbin.org/get')
print(r.text)
#
```
**1.返回的结果包含的内容:请求头 url ip等信息**
**2.json格式的字符窜**

- get请求方式传参数的方式

```python
import requests
data={
    'name':'germey',
    'age':22
}
r=requests.get('http://httpbin.org/get',params=data)
print(r.text)
```

- json字符窜通过`json()`方法解析为字典类型

```python
print(r.json())
```
**如果返回的不是json格式的字符窜，则会抛出异常**

- 添加请求头来抓取网页

```python
import requests
import re

headers={
    'User-Agent':'Mozilla/5.0 (Macintosh;Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML,like Gecko) Chrome/52.0.2743.116 Safari/537.36'
}
r=requests.get('http://www.zhihu.com/explore',headers=headers)
pattern=re.compile('explore-feed.*?question_link.*?>(.*?)</a>',re.S)
titles=re.findall(pattern,r.text)
print(titles)
```

- 抓取二进制数据

- 案列，抓取图片

```python
import requests
r=requests.get("https://github.com/favicon.ico")
print(r.text)
print(r.content)#二进制数据
```

- 保存二进制文件到本地

```python
import requests
r=requests.get("https://github.com/favicon.ico")
with open('favicon.ico','wb') as f:
    f.write(r.content)
```

#### 实现requests的`POST`请求

- 构造post请求

```python
import requests
data={'name':'germey','age':'22'}#post请求
r=requests.post('http://httpbin.org/post',data=data)
print(r.text)
```

- post 响应可以获取到的信息

```python
import requests
r=requests.get('http://www.jianshu.com')
print(type(r.status_code),r.status_code)
print(type(r.headers),r.headers)
print(type(r.cookies),r.cookies)
print(type(r.url),r.url)
print(type(r.history),r.history)
# 输出请求状态码
# 输出响应头
# 输出cookies属性信息
# 输出url
# 输出请求的历史
```

- 判断请求是否成功
> 使用`resquests.code` 内置对象验证请求是否成功
>
- 案例

```python
import requests
r=requests.get('http://www.jianshu.com')
exit() if not r.status_code == requests.codes.ok else print("请求成功")
# 除了ok之外，其他状态码查看状态码表
```

### requests 的高级用法（文件上传，cookies设置，代理设置）

#### 文件上传
```python
import requests
files={'file':open('favicon.ico','rb')}
r=requests.post("http://httpbin.org/post",files=files)
print(r.text)
```

#### 获取并设置cookie

- 通过请求头设置cookie对象

```python
import requests

headers={
    'Cookie':'_zap=38630bcf-b9c5-4e58-9c1e-09cfd83dc2af; d_c0="AMBlfYrRYg6PTgCANgQsXCJ8OMEs587PggE=|1539929767"; q_c1=18559a0e44454dfe94fd68c39d31c833|1539929769000|1539929769000; _xsrf=9gnqMVS5i7XXsEdpB2D4KDR9wyub6wDu; tgw_l7_route=ec452307db92a7f0fdb158e41da8e5d8; capsion_ticket="2|1:0|10:1543542206|14:capsion_ticket|44:MjBmZmI5ZTQ4Y2E5NDA3MWE5OGMzMzA5ZDJmYmRkMTY=|dbcd933f7363a49d8434eeb36c3c416506a505cbda762f60b4eeba2e3f62bcca"; z_c0="2|1:0|10:1543542235|4:z_c0|92:Mi4xWWlQQ0NRQUFBQUFBd0dWOWl0RmlEaVlBQUFCZ0FsVk4yLVB0WEFDbDl3cHkwWXJ1aHgwYXV0dko4djJXNDRSdmlR|dfe7087265e95ccec6847bb099e1b8fafb1f0456a844bd4cd8f8ca6bfb1692eb"; unlock_ticket="ABAk4aO4qA0mAAAAYAJVTeOcAFxDjDjGYw24_na2FJJcqHq6Au2s3g=="; tst=r',
    'Host':'www.zhihu.com',
    'User-Agent':'Mozilla/5.0 (Macintosh;Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML,like Gecko) Chrome/52.0.2743.116 Safari/537.36'
}
r= requests.get('https://www.zhihu.com',headers=headers)
print(r.text)
```

#### 使用`session`维持登陆会话

```python
import requests

s=requests.Session()
s.get('http://httpbin.org/cookies/set/number/123456789')
r=s.get('http://httpbin.org/cookies')
print(r.text)
```

#### SSL证书验证

- 当发送http请求的时候，会进行证书的验证.

- 出现ssl证书错误的告警的处理方式

```python

import requests
import logging

# verify=True/False 是否需要进行ssl验证
response =requests.get('https://www.12306.cn',verify=True)
# 通过捕获异常的方式避免告警
logging.captureWarnings(True)
print(response.status_code)
# 也可以指定本地证书的方式
response=requests.get('https://www.12306.cn',cert=('/path/server.crt','/path/key'))
```

#### 代理的设置

```python
import requests

#设置代理
proxies={
    "http":"http://10.10.1.10:3128",
    "https":"http://10.10.1.10:1080",
}
requests.get("https")
```

![使用代理的注意点](2018-12-3.jpg)

#### 超时设置

> - 1.设置总超时
> - 2.分段设置超时
> - 3.设置永久不超时

```python
import requests

# 设置超时1秒告警 但是这个超时是请求和读取这2个阶段的总和
r=requests.get("https://www.taobao.com",timeout=1)
# 具体的设置
r.requests.get("https://www.taobao.com",timeout=(5,11,30))

# 设置永不超时
r=requests.get("https://www.taobao.com",timeout=None)
print(r.status_code)
```

#### 身份认证的使用方式

```python
# 身份认证，输入用户名和输入密码
import requests
from requests.auth import HTTPBasicAuth #需要引入这个库
r=requests.get('http://kmairlog.kmair.net:8081/system/license',auth=HTTPBasicAuth('admin','oneapm'))
print(r.text)

# 上面这样写太麻烦，简写
#r=requests.get('http://kmairlog.kmair.net:8081',auth=('admin','oneapm'))

#第三方认证方式 OAth方式
#安装 pip3.6 install requests_oauthlib
#使用方式参考官网 https://requests-oauthlib.readthedocs.io/en/latest/
```
#### 通过实例化的数据结构来请求数据

```python
# 将请求表示为数据结构
from requests import Request,Session
#定义请求的地址
url='http://httpbin.org/post'
data={
    'name':'germey'
}
headers={
    'User-Agent':'Mozilla/5.0 (Macintosh;Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML,like Gecko)Chrome/53.0.2785.116 Safari/537.36'
}
s=Session()
req=Request('POST',url,data=data,headers=headers)
prepped=s.prepare_request(req)
r=s.send(prepped)
print(r.text)
```
